# hma_kinesics:  Head Movements Analysis Using Kinesics

![Block diagram of the code.](/assets/images/block_diagram.png)

This repository contains the code for generating kinemes and kineme histograms described in the ICMI 2024 paper titled "Detecting Autism from Head Movements using Kinesics'.
The code reads head movements angles yaw, pitch, and roll (PYR)  and generates kinemes and kineme histograms. The head movements angle sequences (PYR) are generated by fitting a 3D Morphable Model to face images in each frame of the input video.  This process can be done by using [3DI](https://github.com/sariyanidi/3DI), [Dense Head Pose Estimation](https://github.com/1996scarlet/Dense-Head-Pose-Estimation) or [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace) but this step is not included in the provided code. 
The code hma_kines_detections.py produces the smallest units kines by finding the peaks and valleys in the PYR components as local extrema in both time and scale space, using SIFT like scale space feature extaction method. 

The code hma_letter_detection.py generates kinemes, or letters of head movement, by classifiying kines of PYR into one of 26 classes. Each letter corresponds to an elementary unit of head movement such as noding, shaking or tilting, as shown in the table below. In this table P, Y, and R denote pitch, yaw and roll angles of head movements while P, V and N denote peak, valley and none kines detected in these components, respectively. Thus in this coding scheme, the letter 'i' corresponds to a noding of first up and then down, while the letter 'u' corresponds to shaking to right and then left. The file in letter_summaries contains the summary of the video file as a sequence of head movement characters like '--iir--a--vv-yz-xxxu-'. Thus, the code converts the head movements in a given video into a sequence of letters.   
</picture>


|No	|P  |Y 	 |R  | Code |        |No   |P  |Y   |R  | Code |        |No   |P  |Y   |R  | Code |
|----:|---|--- |---|:----:   |    ----    |----:|---|--- |---|:----:|-----|----:|---|--- |---|:----:| 
|0	|P	|P	|P	|**a**|		|9	|V	|P	|P	|**j**|		|18	|N	|P	|P	|**s**|
|1	|P	|P	|V	|**b**|		|10	|V	|P	|V	|**k**|		|19	|N	|P	|V	|**t**|
|2	|P	|P	|N	|**c**|		|11	|V	|P	|N	|**l**|		|20	|N	|P	|N	|**u**|
|3	|P	|V	|P	|**d**|		|12	|V	|V	|P	|**m**|		|21	|N	|V	|P	|**v**|
|4	|P	|V	|V	|**e**|		|13	|V	|V	|V	|**n**|		|22	|N	|V	|V	|**w**|
|5	|P	|V	|N	|**f**|		|14	|V	|V	|N	|**o**|		|23	|N	|V	|N	|**x**|
|6	|P	|N	|P	|**g**|		|15	|V	|N	|P	|**p**|		|24	|N	|N	|P	|**y**|
|7	|P	|N	|V	|**h**|		|16	|V	|N	|V	|**q**|		|25	|N	|N	|V	|**z**|
|8	|P	|N	|N	|**i**|		|17	|V	|N	|N	|**r**|		|26	|N	|N	|N	|**-**|


The code hma_kinemes_detection.py generates higher level of kinemes as group of kinemes such as up-down noding (i) followed by a down-up noding (r) as 'ir'. 

The code hma_letter_histogram.py generates the histograms of kineme groups and and saves to histogram.csv file as final features.

### Introduction 
In this study, we introduce a method to succesfully extract  the basic head movement units such as noding, shaking or tilting from head movement angles by utilizing kinesics pioneered by Birdwhistell. In this approach, we first determine the smallest unit of head movement, called kine, based on the anatomical constraints of the neck and head. We then quantify the location, magnitude, and duration of kines within each angular component of head movement. Through defining possible combinations of identified kines, we define a higher-level construct, kineme, which corresponds to basic head motion units such as nodding and shaking. We validate the proposed framework by predicting autism spectrum disorder (ASD) diagnosis from video recordings of interacting partners. We show that the multi-scale property of the proposed framework provides a significant advantage, as collapsing behavior across temporal scales reduces performance consistently. Finally, we incorporate another fundamental behavioral modality, namely speech, and show that distinguishing between speaking- and listening-time head movements significantly improves ASD classification performance.

### Background and Development
The work was conducted during Muhittin Gokmen's sabbatical leave from MEF University, Turkey, at the Center for Autism Research (CAR) at the Children's Hospital of Philadelphia (CHOP), USA, from September 2023 to September 2024.

### Installation

```
git clone https://github.com/gokmenm/hma_kinesics.git
cd hma_kinesics
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
```

### Usage Example
1. Prepare input file including head movements angles yaw, pitch and roll as in the example file, "sample1.poses_rad". The angles should be given in radians, they are converted to degrees in the code. The speech_labels file, named "sample1.speech_labels", should have 1 if the person is speaking in a timeframe, 0 otherwise. You can use [TalkNet ASD](https://github.com/TaoRuijie/TalkNet-ASD) generate speech labels.
2. After preparing sample1.poses_rad and sample1.speech_labels files, run hma_kinesics.py to generate output files: 
```
python3 hma_kinesics.py --filename_base sample1
```
### Outputs
The code generates the following output folders:    

- letters  
- letter_summaries
- letter_histograms 

If you do not have sample1.speech_labels file; the code generates a dummy file with all speaking labels marked as 1. In this case only nonseparated outputs should be used.

### Citing
If you use this approach or the code for your publication, please cite as 
```
@inproceedings{Gokmen2024,
   author = {Muhittin Gokmen and Evangelos Sariyanidi and Lisa Yankowitz and Casey J Zampella and Robert T. Schultz and Birkan Tun√ß},
   doi = {10.1145/3678957.3685711},
   isbn = {9798400704628},
   booktitle = {Proc. of the 26th ACM International Conference on Multimodal Interaction (ICMI'2024)},
   keywords = {Autism,Computer Vision,Head Movements,Kinesics,Psychology},
   title = {Detecting Autism from Head Movements using Kinesics},
   url = {https://doi.org/10.1145/3678957.3685711},
   year = {2024},
}
```
